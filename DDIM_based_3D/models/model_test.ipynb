{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SpatialTemporal_Diffusion import *\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat, pack , unpack\n",
    "import sys, os\n",
    "module_path = os.path.abspath(os.path.join('..', 'einops_exts'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from einops_exts import check_shape, rearrange_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10800, 2, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "optical_flow = np.load('/data/liaohx/voxelmorph/output_0718_7/warp.npy')\n",
    "print(optical_flow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(in_channels):\n",
    "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x):  # 这里的输入是3帧\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
    "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
    "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
    "\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
    "        w_ = rearrange(w_, 'b i j -> b j i')\n",
    "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
    "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 3])\n",
      "torch.Size([2, 3, 25, 4])\n",
      "torch.Size([2, 25, 25, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "# 假设 q_center 和 k_neighbour 是随机初始化的张量\n",
    "b, c, t, h, w = 2, 3, 4, 5, 5\n",
    "q_center = torch.randn(b, c, h, w)\n",
    "k_neighbour = torch.randn(b, c, t, h, w)\n",
    "\n",
    "# 重新排列张量\n",
    "q_center = rearrange(q_center, 'b c h w -> b (h w) c')\n",
    "k_neighbour = rearrange(k_neighbour, 'b c t h w -> b c (h w) t')\n",
    "\n",
    "# 检查形状\n",
    "print(q_center.shape)  # 应该是 (b, h*w, c)\n",
    "print(k_neighbour.shape)  # 应该是 (b, c, h*w, t)\n",
    "\n",
    "# 计算注意力权重\n",
    "w_ = torch.einsum('bic, bcht -> biht', q_center, k_neighbour)\n",
    "\n",
    "# 检查结果形状\n",
    "print(w_.shape)  # 应该是 (b, h*w, h*w, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(in_channels):\n",
    "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x_center, x_neighbour, x):\n",
    "        # the input shape of x should be (B, C, t, h w)  \n",
    "        B, C, t, h, w = x.shape\n",
    "        # to find the neighbour tensor for each frame\n",
    "        for frame in range(x.shape[2]):\n",
    "            \n",
    "        \n",
    "        \n",
    "        b1, c, h, w = x_center.shape\n",
    "        b3, c, t, h, w = x_neighbour.shape\n",
    "        \n",
    "        h_center = x_center\n",
    "        h_neighbour = x_neighbour\n",
    "        \n",
    "        h_center = self.norm(h_center)\n",
    "        h_neighbour = self.norm(h_neighbour)\n",
    "        \n",
    "        q_center = self.q(h_center)  # b1, c, h, w\n",
    "        \n",
    "        k_neighbour = self.k(h_neighbour)  # b3, c, h, w\n",
    "        v_neighbour = self.v(h_neighbour)  # b3, c, h, w\n",
    "        \n",
    "        # 将 q_center 和 k_neighbour 沿着第一个维度进行广播再乘到一起去\n",
    "        q_center = rearrange(q_center, 'b c h w -> b (h w) c')\n",
    "        k_neighbour = rearrange(k_neighbour, 'b c t h w -> b c (hw) t')\n",
    "        \n",
    "        # 沿着时间维度进行广播再计算attention操作\n",
    "        w_ = torch.einsum('bij, bjkt -> bikt', q_center, k_neighbour)  # the shape should be like (b hw hw t)\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)  \n",
    "        # attend to values\n",
    "        v_ = rearrange(v_neighbour, 'b c t h w -> b c (h w) t')\n",
    "        w_ = rearrange(w_, 'b i j t -> b j i t')\n",
    "        \n",
    "        h_ = torch.einsum('bijt, bjkt -> bikt', v_, w_)  # b c hw t \\times b hw hw t -> b c hw t\n",
    "        h_ = rearrange()\n",
    "        \n",
    "        \n",
    "        h_ = self.norm(h_) \n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
    "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
    "        w_ = torch.einsum('bij,bjk->bik', q, k)  # b (hw) c \\times b c (hw) -> b (hw) (hw)\n",
    "\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
    "        w_ = rearrange(w_, 'b i j -> b j i')  # swap two axis??  \n",
    "        \n",
    "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
    "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Data Consistency Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance the temporal consistency for denoised initial prediction\n",
    "class TemporalConsistencyBlock(nn.Module):\n",
    "    \"\"\"docstring for TemporalConsistencyBlock.\"\"\"\n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
